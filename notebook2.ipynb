{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicción del abandono de empleados sobre los datos de competition\n",
    "### Paulo Álvarez Da Costa. NIA: 100475757\n",
    "\n",
    "Usaré el dataset de employee_competition proporcionado para predecir si un empleado abandonará la empresa o no.\n",
    "\n",
    "#### Configuraciones iniciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga librerias necesarias\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Datos usados para la predicción\n",
    "employees_comp = pd.read_csv(\"DATA/employee_competition.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limpieza y preprocesamiento de datos en competition\n",
    "\n",
    "Voy a comenzar realizando el mismo proceso de limpieza y preprocesamiento que en el notebook anterior, para garantizar que el modelo pueda transformar correctamente las columnas del dataset 'competition', ya que el pipeline del modelo espera exactamente las mismas variables (nombres, tipos y orden). Si este paso no se realizara, el modelo no podría predecir los datos de 'competition', ya que no se le ha enseñado a transformar las columnas de este dataset. Por lo tanto, es fundamental que el preprocesamiento sea idéntico al del dataset 'train' para asegurar la compatibilidad y precisión del modelo en las predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma de los datos preprocesados: (1470, 46)\n"
     ]
    }
   ],
   "source": [
    "# Eliminar las mismas columnas que se eliminaron en el notebook1 \n",
    "employees_comp.drop(columns=['EmployeeCount', 'Over18', 'StandardHours', 'EmployeeID'], inplace=True)\n",
    "\n",
    "# Identificar columnas categóricas y numéricas\n",
    "categorical_columns = employees_comp.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "numeric_columns = employees_comp.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "\n",
    "# Pipeline para transformar las variables categóricas\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Imputa valores nulos con la moda\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # Transforma variables categóricas a numéricas\n",
    "])\n",
    "\n",
    "# Pipeline para transformar las variables numéricas\n",
    "numeric_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Imputa valores nulos con la mediana\n",
    "    ('scaler', RobustScaler())                      # Escala las variables numéricas\n",
    "])\n",
    "\n",
    "# Combinar ambos pipelines en un ColumnTransformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_pipeline, numeric_columns),\n",
    "    ('cat', categorical_pipeline, categorical_columns)\n",
    "])\n",
    "\n",
    "# Aplicar el preprocesamiento a las features\n",
    "employees_comp_preprocessed = preprocessor.fit_transform(employees_comp)\n",
    "\n",
    "print(\"Forma de los datos preprocesados:\", employees_comp_preprocessed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar, el dataset preprocesado presenta 46 columnas, las mismas que el dataset de entrenamiento, lo que garantiza que el modelo pueda realizar predicciones sin problemas.\n",
    "\n",
    "#### Predicción final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones guardadas en 'predicciones.csv'\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo final\n",
    "modelo_final = joblib.load(\"modelo_final.pkl\")\n",
    "\n",
    "# Realizar predicciones (0 = No, 1 = Yes)\n",
    "predicciones_binarias = modelo_final.predict(employees_comp_preprocessed)\n",
    "\n",
    "# Mapear a 'Yes' y 'No'\n",
    "predicciones = pd.Series(predicciones_binarias).map({1: 'Yes', 0: 'No'})\n",
    "\n",
    "# Guardar predicciones en CSV\n",
    "df_resultado = pd.DataFrame({'Attrition': predicciones})\n",
    "df_resultado.to_csv(\"predicciones.csv\", index=False)\n",
    "\n",
    "print(\"Predicciones guardadas en 'predicciones.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
